import{_ as n,c as a,d as e,o as t}from"./app-DDoPRsSr.js";const p={};function c(i,s){return t(),a("div",null,[...s[0]||(s[0]=[e(`<h1 id="用pytorch进行混合精度训练-推理" tabindex="-1"><a class="header-anchor" href="#用pytorch进行混合精度训练-推理"><span>用PyTorch进行混合精度训练/推理</span></a></h1><p>最近尝试了用混合精度的方法来加速模型训练。要用PyTorch进行混合精度训练非常简单，因为PyTorch已经封装好了这个功能，API也很便捷，要import的东西只有下面这些：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>amp <span class="token keyword">import</span> autocast<span class="token punctuation">,</span> GradScaler</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>要启用混合精度训练，对原有代码的改动也很少。首先是把前向传播的代码用<code>autocast</code>包裹起来：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">with</span> autocast<span class="token punctuation">(</span><span class="token string">&#39;cuda&#39;</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span></span>
<span class="line">    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>然后在反向传播的时候，用<code>GradScaler</code>来缩放梯度：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">scaler <span class="token operator">=</span> GradScaler<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># In the training loop:</span></span>
<span class="line">scaler<span class="token punctuation">.</span>scale<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">scaler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span></span>
<span class="line">scaler<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="实验结果" tabindex="-1"><a class="header-anchor" href="#实验结果"><span>实验结果</span></a></h2><p>我在一台配备RTX 4090的机器上进行了实验，发现对于GPT-2，混合精度训练确实能够大幅提升训练速度，但是对于普通的全连接层或者卷积层，提升效果并不明显。我的解释是：Transformer模型的计算量远大于全连接层或者卷积层，这样才能体现出混合精度训练的优势。</p>`,9)])])}const l=n(p,[["render",c]]),r=JSON.parse('{"path":"/posts/pytorch-amp.html","title":"用PyTorch进行混合精度训练/推理","lang":"zh-CN","frontmatter":{"date":"2025-02-28T00:00:00.000Z","tag":["神经网络"]},"headers":[{"level":2,"title":"实验结果","slug":"实验结果","link":"#实验结果","children":[]}],"git":{"updatedTime":1763698804000,"contributors":[{"name":"shi0rik0","username":"shi0rik0","email":"anguuan@outlook.com","commits":1,"url":"https://github.com/shi0rik0"}],"changelog":[{"hash":"27621ea60c6645dc9e44007e7ab6fd2858c37eaf","time":1763698804000,"email":"anguuan@outlook.com","author":"shi0rik0","message":"Migrate posts"}]},"filePathRelative":"posts/pytorch-amp.md","excerpt":"\\n<p>最近尝试了用混合精度的方法来加速模型训练。要用PyTorch进行混合精度训练非常简单，因为PyTorch已经封装好了这个功能，API也很便捷，要import的东西只有下面这些：</p>\\n<div class=\\"language-python line-numbers-mode\\" data-highlighter=\\"prismjs\\" data-ext=\\"py\\"><pre><code><span class=\\"line\\"><span class=\\"token keyword\\">from</span> torch<span class=\\"token punctuation\\">.</span>amp <span class=\\"token keyword\\">import</span> autocast<span class=\\"token punctuation\\">,</span> GradScaler</span>\\n<span class=\\"line\\"></span></code></pre>\\n<div class=\\"line-numbers\\" aria-hidden=\\"true\\" style=\\"counter-reset:line-number 0\\"><div class=\\"line-number\\"></div></div></div>"}');export{l as comp,r as data};
