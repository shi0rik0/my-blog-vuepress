import{_ as n,c as a,d as e,o as p}from"./app-D9VVartN.js";const t={};function i(l,s){return p(),a("div",null,[...s[0]||(s[0]=[e(`<h1 id="使用docker搭建pyspark学习环境" tabindex="-1"><a class="header-anchor" href="#使用docker搭建pyspark学习环境"><span>使用Docker搭建PySpark学习环境</span></a></h1><p>首先安装 Docker，然后拉取 Spark 的 Docker 镜像：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">docker pull apache/spark</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>然后创建一个 Docker 容器：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">docker run -d -u root apache/spark tail -f /dev/null</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>（注：这里的-u root 的作用是把容器的默认用户设置成 root，如果不这么做，VSCode 就无法附加到容器上。）</p><p>接着就可以使用 VSCode 进入容器。进入后在终端里打开 spark-shell：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">/opt/spark/bin/spark-shell</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>打开之后会看到类似下面的输出，里面包含了 Spark 的版本。这里我的版本是 3.5.1。</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">Welcome to</span>
<span class="line">      ____              __</span>
<span class="line">     / __/__  ___ _____/ /__</span>
<span class="line">    _\\ \\/ _ \\/ _ \`/ __/  &#39;_/</span>
<span class="line">   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1</span>
<span class="line">      /_/</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>然后安装对应版本的 PySpark：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">pip install pyspark==3.5.1</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><strong>注意：这里的 PySpark 版本一定要和 Spark 版本一致。如果不一致，并不会立刻报错，而是会出现有些代码可以运行，有些代码不能运行的情况！</strong></p><p>安装完 PySpark 之后，就可以用 Python 编写 Spark 程序了。下面是一个简单的例子：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext</span>
<span class="line"></span>
<span class="line">sc <span class="token operator">=</span> SparkContext<span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&#39;apple&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;apple&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;banana&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;banana&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;banana&#39;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> x <span class="token operator">+</span> y<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>你应该会看到类似下面的输出：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">[(&#39;banana&#39;, 3), (&#39;apple&#39;, 2)]</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div>`,17)])])}const c=n(t,[["render",i]]),o=JSON.parse('{"path":"/posts/2024/06/pyspark-docker-env.html","title":"使用Docker搭建PySpark学习环境","lang":"zh-CN","frontmatter":{"date":"2024-06-20T00:00:00.000Z"},"headers":[],"git":{"updatedTime":1763699399000,"contributors":[{"name":"shi0rik0","username":"shi0rik0","email":"anguuan@outlook.com","commits":2,"url":"https://github.com/shi0rik0"}],"changelog":[{"hash":"35d50711ef3a591a2383977aa02873c707aa5c1e","time":1763699399000,"email":"anguuan@outlook.com","author":"shi0rik0","message":"Organize files by date"},{"hash":"27621ea60c6645dc9e44007e7ab6fd2858c37eaf","time":1763698804000,"email":"anguuan@outlook.com","author":"shi0rik0","message":"Migrate posts"}]},"filePathRelative":"posts/2024/06/pyspark-docker-env.md","excerpt":"\\n<p>首先安装 Docker，然后拉取 Spark 的 Docker 镜像：</p>\\n<div class=\\"language-text line-numbers-mode\\" data-highlighter=\\"prismjs\\" data-ext=\\"text\\"><pre><code><span class=\\"line\\">docker pull apache/spark</span>\\n<span class=\\"line\\"></span></code></pre>\\n<div class=\\"line-numbers\\" aria-hidden=\\"true\\" style=\\"counter-reset:line-number 0\\"><div class=\\"line-number\\"></div></div></div>"}');export{c as comp,o as data};
