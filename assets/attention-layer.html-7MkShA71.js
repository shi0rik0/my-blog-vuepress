import{_ as $,c as e,d as a,o as n}from"./app-D9VVartN.js";const i={};function o(p,t){return n(),e("div",null,[...t[0]||(t[0]=[a('<h1 id="attention层的简单讲解" tabindex="-1"><a class="header-anchor" href="#attention层的简单讲解"><span>Attention层的简单讲解</span></a></h1><h2 id="输入与输出的维度" tabindex="-1"><a class="header-anchor" href="#输入与输出的维度"><span>输入与输出的维度</span></a></h2><p>Attention 层是用来处理序列输入的，所以它接受的输入是一个二维的$n \\times N_i$的矩阵，而输出则是一个$n \\times N_o$的矩阵。$n$是输入序列的 token 个数，$N_i$则是 embedding 向量的长度，$N_o$则是输出的状态向量的长度。理论上 Attention 层对于$n$没有限制，而$N_i$和$N_o$则是模型的超参数。</p><h2 id="attention-层的超参数" tabindex="-1"><a class="header-anchor" href="#attention-层的超参数"><span>Attention 层的超参数</span></a></h2><p>除了$N_i$和$N_o$以外，Attention 层还有其他超参数。</p><p>一个 Attention 层包含 3 个全连接层，分别叫做 Query、Key 和 Value，用符号$Q$、$K$和$V$表示。这 3 个全连接层的输入维度都是$N_i$，而输出维度都是超参数，分别为$D_q$、$D_k$和$D_v$。稍后会看到，Attention 层要求$D_q=D_k$以及$D_v=N_o$。</p><p>综上所述，Attention 层的超参数有$N_i$、$N_o$和$D_q$。</p><h2 id="推理过程" tabindex="-1"><a class="header-anchor" href="#推理过程"><span>推理过程</span></a></h2><p>我们将输入的向量依次记作$x_1$、$x_2$……$x_n$，同理输出的向量记作$y_1$、$y_2$……$y_n$。</p><p>推理的时候，首先要将输入向量都经过全连接层一次，得到$Q(x_1)$、$K(x_1)$、$V(x_1)$等等。最终的输出是由$V$的结果加权求和得到的：</p><p>$$ y_i = \\sum_j \\alpha_{ij} V(x_j), \\forall i $$</p><p>因为$\\alpha_{ij}$是权重，所以自然要满足：</p><p>$$ \\sum_j \\alpha_{ij} = 1,\\forall i $$</p><p>$\\alpha_{ij}$是根据$Q$和$K$的结果得到的：</p><p>$$ \\alpha^\\prime_{ij} = \\frac{Q(x_i) \\cdot K(x_j)}{\\sqrt{D_q}}, \\forall i, j $$</p><p>这里我用的符号是$\\alpha^\\prime$而非$\\alpha$，因为$\\alpha^\\prime$不满足权重和为 1 的要求，因此不是最终结果。要让和为 1，使用 softmax 层处理一下即可。</p><p>（计算$\\alpha$要除以$\\sqrt{D_q}$是为了归一化。）</p>',17)])])}const s=$(i,[["render",o]]),l=JSON.parse('{"path":"/posts/2024/10/attention-layer.html","title":"Attention层的简单讲解","lang":"zh-CN","frontmatter":{"date":"2024-10-02T00:00:00.000Z"},"headers":[{"level":2,"title":"输入与输出的维度","slug":"输入与输出的维度","link":"#输入与输出的维度","children":[]},{"level":2,"title":"Attention 层的超参数","slug":"attention-层的超参数","link":"#attention-层的超参数","children":[]},{"level":2,"title":"推理过程","slug":"推理过程","link":"#推理过程","children":[]}],"git":{"updatedTime":1763699399000,"contributors":[{"name":"shi0rik0","username":"shi0rik0","email":"anguuan@outlook.com","commits":2,"url":"https://github.com/shi0rik0"}],"changelog":[{"hash":"35d50711ef3a591a2383977aa02873c707aa5c1e","time":1763699399000,"email":"anguuan@outlook.com","author":"shi0rik0","message":"Organize files by date"},{"hash":"27621ea60c6645dc9e44007e7ab6fd2858c37eaf","time":1763698804000,"email":"anguuan@outlook.com","author":"shi0rik0","message":"Migrate posts"}]},"filePathRelative":"posts/2024/10/attention-layer.md","excerpt":"\\n<h2>输入与输出的维度</h2>\\n<p>Attention 层是用来处理序列输入的，所以它接受的输入是一个二维的$n \\\\times N_i$的矩阵，而输出则是一个$n \\\\times N_o$的矩阵。$n$是输入序列的 token 个数，$N_i$则是 embedding 向量的长度，$N_o$则是输出的状态向量的长度。理论上 Attention 层对于$n$没有限制，而$N_i$和$N_o$则是模型的超参数。</p>\\n<h2>Attention 层的超参数</h2>\\n<p>除了$N_i$和$N_o$以外，Attention 层还有其他超参数。</p>\\n<p>一个 Attention 层包含 3 个全连接层，分别叫做 Query、Key 和 Value，用符号$Q$、$K$和$V$表示。这 3 个全连接层的输入维度都是$N_i$，而输出维度都是超参数，分别为$D_q$、$D_k$和$D_v$。稍后会看到，Attention 层要求$D_q=D_k$以及$D_v=N_o$。</p>"}');export{s as comp,l as data};
